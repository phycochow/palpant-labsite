<div class="qa-container">
  <div class="qa-header">
    <h2>Frequently Asked Questions</h2>
    <p>Common questions about CMPortal's methodology, data handling, and interpretation guidelines.</p>
  </div>

  <!-- Reviewer #1 Section -->
  <div class="qa-section">
    <h3 class="section-title">Data Quality & Measurement Methods</h3>
    
    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        Don't different measurement methods (PCR vs RNA-seq, Seahorse vs single-parameter assays) introduce technical variability that biases enrichment results?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p><strong>General Answer:</strong> No. CMPortal doesn't compare molecular data across studies. Measurement methods are recorded only as metadata describing what each study reported, not as quantitative inputs. The analysis evaluates protocol design variables (media, supplements, cell lines) against maturity outcomes, not the depth of molecular characterization.</p>
          <p><strong>Technical Answer:</strong> All measurement descriptors are one-hot encoded as independent binary features (e.g., "Gene Analysis Method – PCR", "Gene Analysis Method – RNA-seq"). This prevents methods of differing resolution from being conflated. The random forest models evaluate protocol variables' association with physiological endpoints (contractile force, sarcomere length), not molecular readouts themselves. A study reporting RNA-seq doesn't gain statistical weight over one reporting PCR—only the protocol design choices are modeled.</p>
          <p><strong>Interface Update:</strong> Users can now toggle to hide descriptive/reporting-level enrichments, focusing solely on causal protocol variables.</p>
        </div>
      </div>
    </div>

    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        Won't studies with richer molecular characterization dominate enrichment results?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p><strong>General Answer:</strong> No. CMPortal performs protocol benchmarking, not molecular benchmarking. The depth of molecular characterization doesn't affect maturity endpoints used in the analysis.</p>
          <p><strong>Technical Answer:</strong> All enrichment analyses use the one-hot encoded Supplemental Table 1, where each protocol feature is evaluated independently via permutation-based random forest. Information gain is calculated per feature regardless of how many sub-categories exist. Because analysis operates on protocol descriptors (not cell-based molecular outputs), molecular dataset richness cannot bias maturity endpoint associations. Sensitivity analysis removing all measurement-method variables showed stability of top biological features.</p>
        </div>
      </div>
    </div>

    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        Does inconsistent reporting across studies introduce noise that undermines cross-protocol comparisons?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p><strong>General Answer:</strong> Inconsistent reporting is a field-wide challenge. CMPortal's unsupervised framework minimizes this by evaluating features independently rather than requiring complete reporting across all studies.</p>
          <p><strong>Technical Answer:</strong> Jaccard index analysis (Figure S1) confirmed data structure preservation after encoding. Random forest models evaluate features individually, so missing categories don't disproportionately affect results. Permutation testing (10,000 iterations) ensures only robust associations survive significance filtering. Expected biological enrichments (Wnt modulation, metabolic substrates, matrix stiffness) emerged consistently, and experimental validation confirmed predictions.</p>
        </div>
      </div>
    </div>
  </div>

  <!-- Sparse Data Section -->
  <div class="qa-section">
    <h3 class="section-title">Sparse Data & Reporting Frequency</h3>

    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        Can variables reported in only 2-3 studies be reliably compared to frequently reported features?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p><strong>General Answer:</strong> Variables reported fewer than three times are labeled "Rare/Specialized" to flag their limited representation. The analysis framework treats each feature independently, so rare features don't gain undue influence simply by being infrequent.</p>
          <p><strong>Technical Answer:</strong> One-hot encoding evaluates features independently. Figure 1D–E shows rare features don't systematically inflate enrichment rates. For example, BJ1 cell line (7 studies) enriched in top contractility quantile despite many cell lines having higher representation. Information gain and permutation testing ensure only statistically supported associations emerge. Supplementary Figure XX demonstrates no correlation between reporting frequency and enrichment rank.</p>
        </div>
      </div>
    </div>

    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        Doesn't treating missing data as informative risk overinterpretation?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p><strong>General Answer:</strong> Missing entries reflect design choices (whether a protocol included specific elements), not arbitrary absence. For protocol variables, "not reported" typically means "not used."</p>
          <p><strong>Technical Answer:</strong> CMPortal evaluates protocol design variables, not molecular outputs. Missing features represent unutilized protocol components (e.g., no Wnt modulation, no fatty acid supplementation). Random forest impurity models evaluate presence/absence independently, with directionality quantified via log-odds ratios. Significance derives from 10,000 permuted datasets, ensuring associations are robust to resampling under identical sparsity conditions.</p>
        </div>
      </div>
    </div>

    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        Would sensitivity analyses for sparsity strengthen the findings?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p><strong>General Answer:</strong> The permutation-based significance scoring already provides this. Training on thousands of permuted datasets directly tests sensitivity to data sparsity.</p>
          <p><strong>Technical Answer:</strong> Enrichment confidence values in Figure 1D–E reflect stability under data sparsity and reporting noise. The 10,000-permutation approach distinguishes stable associations from chance findings in sparse dataset regions. This built-in sensitivity evaluation addresses the underlying concern without requiring separate sparsity-specific tests.</p>
        </div>
      </div>
    </div>

    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        Don't highly reported features like Matrigel statistically dominate enrichment results?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p><strong>General Answer:</strong> No. Enrichment depends on consistent co-occurrence with maturity quantiles, not reporting frequency.</p>
          <p><strong>Technical Answer:</strong> One-hot encoding evaluates all features independently—reporting frequency doesn't influence enrichment score or random forest information gain. Figure 1D–E shows several top enrichments correspond to low-frequency features (e.g., BJ1 cell line). Permutation-based significance scoring (10,000 datasets) provides additional safeguard. Supplementary Figure XX demonstrates no correlation between reporting frequency and enrichment strength.</p>
        </div>
      </div>
    </div>
  </div>

  <!-- Quantile Binning Section -->
  <div class="qa-section">
    <h3 class="section-title">Quantile Binning & Data Categorization</h3>

    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        Doesn't binning continuous variables lose biological meaning and introduce arbitrary cutoffs?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p><strong>General Answer:</strong> Quantile binning is necessary because the field lacks standardized thresholds for most maturity metrics. Reporting practices vary widely, making absolute numeric values incomparable across studies.</p>
          <p><strong>Technical Answer:</strong> One-hot encoding requires discrete categories—continuous values cannot be encoded directly. Quantile binning provides unsupervised, data-driven transformation that balances datasets evenly (±1 protocol per bin) while maximizing distinguishable groups. Pairwise correlation analyses (Table S3) validate continuous relationships without relying on binning: conduction velocity vs resting membrane potential (ρ = –1, p < 0.01), contractile stress vs force (ρ = 0.87, p < 0.001), and differentiation purity vs sarcomere length (ρ = 0.33, p < 0.05) all recapitulate known cardiomyocyte physiology.</p>
        </div>
      </div>
    </div>

    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        What do Q1–Q5 categories mean?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p><strong>General Answer:</strong> Q1 = top-performing quantile for a given metric, Q5 = lowest-performing, Q2–Q4 = intermediate ranges. Categories are algorithmically determined to maximize balanced representation.</p>
          <p><strong>Technical Answer:</strong> Supplemental Table XX provides quantitative ranges linking each Q-bin to physiological values for every parameter. Algorithm optimizes bin count to maintain ±1 protocol per bin, enabling maximum resolution given available data. This ensures meaningful relative performance comparisons across heterogeneous studies.</p>
        </div>
      </div>
    </div>
  </div>

  <!-- Validation & Interpretation Section -->
  <div class="qa-section">
    <h3 class="section-title">Experimental Validation & Interpretation</h3>

    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        Is the Frank-Starling independence claim overgeneralized from limited validation?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p><strong>General Answer:</strong> We've revised to state that contractility and sarcomere length are "decoupled" rather than "independent." Our validation provides proof-of-principle that CMPortal generates testable hypotheses.</p>
          <p><strong>Technical Answer:</strong> Database analysis across 300+ protocols showed no correlation between sarcomere length and contractile force (ρ not significant). Experimental validation using DMEM+fatty acids vs RPMI+B27 demonstrated increased contractility without sarcomere length change—consistent with database prediction. While fatty acids are known to improve contractility, no prior study examined whether this occurs independently of structural maturation. Our finding challenges the untested assumption that these metrics co-develop.</p>
        </div>
      </div>
    </div>

    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        What other "unexpected findings" does CMPortal reveal?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p>Table S3 reveals several non-intuitive correlations: sarcomere length doesn't correlate with contractile force/stress despite traditional assumptions; fibroblast ratio positively associates with longer sarcomeres (ρ = 0.72, p < 0.001) but negatively with calcium relaxation (ρ = –0.75, p < 0.05); beat rate strongly negatively correlates with insulin withdrawal duration (ρ = –0.78, p < 0.05). These context-dependent patterns extend beyond established maturation paradigms.</p>
        </div>
      </div>
    </div>

    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        How does CMPortal reconcile with Ewoldt et al.'s finding that no maturation strategy consistently outperforms others?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p><strong>General Answer:</strong> CMPortal doesn't contradict Ewoldt et al.—it extends those findings. While individual variables rarely show uniform effects, specific combinations of protocol features reproducibly associate with distinct outcomes.</p>
          <p><strong>Technical Answer:</strong> Ewoldt et al. demonstrated substantial heterogeneity and inconsistent reporting across maturation strategies. CMPortal reveals context-dependent enrichment patterns inaccessible through conventional comparative analyses. The framework identifies how protocol combinations influence specific maturation dimensions, providing actionable insights despite overall heterogeneity.</p>
        </div>
      </div>
    </div>
  </div>

  <!-- Engineering & Platform Section -->
  <div class="qa-section">
    <h3 class="section-title">Engineering Platforms & Protocol Variables</h3>

    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        How does CMPortal handle engineering platforms, cell density, duration, and concentration ranges?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p><strong>General Answer:</strong> Engineering variables (2D vs 3D, scaffold type, tensioning, tissue size, cell density) are incorporated as independent descriptors when reported in literature. Duration and concentration parameters are treated as continuous variables.</p>
          <p><strong>Technical Answer:</strong> Platform variables appear in enrichment framework when available. Duration-dependent variables (Wnt induction duration, insulin withdrawal, media exchange schedules) and concentration ranges contribute to impurity-based modeling and permutation significance scoring. The framework detects when these parameters meaningfully associate with maturation outcomes. Note: Comprehensive re-review of 300+ studies would be required to systematically add new data categories—current version balances immediate utility with scope limitations.</p>
        </div>
      </div>
    </div>

    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        Why isn't the metabolic switch prioritized given its recognized importance?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p><strong>General Answer:</strong> This reflects data representation limitations, not biological insignificance. "Metabolic Maturation" annotation from Ewoldt et al. was applied only when studies explicitly stated metabolic strategy, but other studies used similar factors without that label.</p>
          <p><strong>Technical Answer:</strong> Ewoldt et al. reported no maturation technique (including metabolic) shows statistically significant superiority—reflecting heterogeneity in factor selection and concentration. Limitations section (lines XXX) now cautions users about interpretation.</p>
        </div>
      </div>
    </div>
  </div>

  <!-- Platform & Updates Section -->
  <div class="qa-section">
    <h3 class="section-title">Platform Updates & Statistical Methods</h3>

    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        How frequently is CMPortal updated with new datasets?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p>Active updates planned annually. Student consortium established to assist data curation through university coursework (journal clubs). Expansion planned for 2026 to include atrial cardiomyocytes and other lineages.</p>
        </div>
      </div>
    </div>

    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        How are Fisher's exact test and machine-learning feature importance integrated?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p><strong>General Answer:</strong> They're complementary. Machine learning quantifies how strongly outcomes differ when a variable is used vs not used. Fisher's exact test evaluates whether a variable is statistically overrepresented in the highest maturity quantile.</p>
          <p><strong>Technical Answer:</strong> Both provide independent evidence of association strength and significance. Agreement isn't required—they assess different statistical properties of the same associations.</p>
        </div>
      </div>
    </div>

    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        Does the permutation approach account for feature interactions?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p>No—each feature is treated independently. Values within each variable column are randomly shuffled to generate perturbed datasets forming null distributions. This allows assessment of each feature's independent association with maturation outcomes.</p>
        </div>
      </div>
    </div>

    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        Why use quantile bins rather than alternative categorization methods?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p>Algorithm optimizes bin number to maximize resolution while maintaining balanced protocol representation. Alternative approaches don't account for balanced bins or address uneven data distribution (many protocols are left-skewed toward immature outcomes). The method ensures maximum interpretability given data sparsity.</p>
        </div>
      </div>
    </div>
  </div>

  <!-- Maturation Independence Section -->
  <div class="qa-section">
    <h3 class="section-title">Maturation Metrics & Independence</h3>

    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        Can you clarify the conclusion about independent modulation of maturation parameters?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p><strong>General Answer:</strong> Recent literature (Fetterman et al.) shows hPSC-CM maturity is compartmentalized. CMPortal supports this: protocol variables driving one maturity metric often differ from those driving others.</p>
          <p><strong>Technical Answer:</strong> Maturation metrics showed limited correlation. Contractile force associated with tensioned 3D constructs, DMEM media, and specific Wnt patterns. Sarcomere length, calcium kinetics, and conduction velocity each linked to different protocol features. This indicates maturation isn't governed by a single unified axis but reflects multiple partially independent pathways. Optimizing one property doesn't guarantee simultaneous enhancement of others.</p>
        </div>
      </div>
    </div>
  </div>

  <!-- UMAP & Visualization Section -->
  <div class="qa-section">
    <h3 class="section-title">Data Visualization & Analysis</h3>

    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        What drives the distinct clusters in the UMAP plot (Figure 1B)?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p>New analysis (Supplementary Figure XX) extracts key database features driving UMAP separation. Results section (lines XXX) provides general description. Brief Communication format limits detailed expansion.</p>
        </div>
      </div>
    </div>

    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        How are "best" and "worst" values defined in Figure 1C?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p><strong>General Answer:</strong> Protocols are rank-ordered by reported values for each metric and divided into maximally balanced quantiles. Highest-value quantile = "best", lowest-value quantile = "worst".</p>
          <p><strong>Technical Answer:</strong> Supplemental Table XX provides quantitative ranges linking Q-bins to physiological values for every parameter. Polynomial model illustrates how significant enrichments scale with sample size—not a mechanistic relationship but statistical demonstration that sufficient database diversity exists to recover meaningful associations. Number of recoverable features plateaus at ~20–30, providing robustness justification.</p>
        </div>
      </div>
    </div>

    <div class="qa-card" onclick="this.classList.toggle('open')">
      <div class="card-header">
        Why do measurement tools like traction force microscopy appear enriched?
      </div>
      <div class="card-body">
        <div class="answer-content">
          <p><strong>General Answer:</strong> Measurement tools describe how contractility was quantified, not causal factors. They appear because they report co-occurrence patterns.</p>
          <p><strong>Technical Answer:</strong> Biologically manipulable protocol components (media, supplements, cell sources, culture geometry) should be interpreted as candidate causal levers. Measurement modalities report co-occurrence but aren't causal. New CMPortal toggle restricts display to putative causal variables, hiding descriptive/methodological features. Supplemental Table XX provides numeric boundaries for all Q-bins.</p>
        </div>
      </div>
    </div>
  </div>

</div>